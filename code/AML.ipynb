{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pytorch_pretrained_bert import BertConfig, BertTokenizer, BertModel, BertForMaskedLM\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import random\n",
    "import matplotlib.pyplot as plt   #jupyter要matplotlib.pyplot\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "伪代码：\n",
    "train_tokens = [], train_labels = [], valid_tokens = [], valid_labels = [], test_tokens=[]\n",
    "初始化的BERT+TextCNN模型M(θ)，θ表示模型参数\n",
    "读取训练数据train_datas\n",
    "train_datas预处理并更新\n",
    "\n",
    "#分词得到BERT的输入\n",
    "for k ← 1, 2, ..., N do\n",
    "    train_datas['features'][k]='[CLS]'+ train_datas[k] + '[SEP]'\n",
    "    x = tokenize(train_datas[k]) \n",
    "    if len(x)>300\n",
    "        x = x[0:300]\n",
    "    #padding处理\n",
    "    while len(x)<300\n",
    "        x = x + '0'\n",
    "    y = train_datas['target'][k]\n",
    "    train_tokens ← x∪tokens\n",
    "    train_labels ← y∪labels\n",
    "    #对验证集和测试集处理相同\n",
    "    \n",
    "end\n",
    "\n",
    "将tokens和对应labels装入batch中得到Dataload\n",
    "定义交叉熵损失HCE(.)，优化器O(.)\n",
    "#输入模型\n",
    "for epoch ← 1, 2, ... do\n",
    "    for b ← 1, 2, ... do\n",
    "        logits ← M(Dataload[b]; θ)\n",
    "        loss ← HCE(logits, Dataload[b]) \n",
    "        θ ← O(θ)\n",
    "    end\n",
    "end\n",
    "得到训练好的模型M(θ')\n",
    "prob ← M(valid_tokens;θ')\n",
    "labels_predict ← argmax(prob)\n",
    "acc ← sum(labels_predict == valid_labels) / len(valid_labels)\n",
    "选取效果最好的训练模型M(θ'')\n",
    "\n",
    "prob ← M(test_tokens;θ')\n",
    "labels_predict ← argmax(prob)\n",
    "储存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_init(datas, mode='train'):\n",
    "    datas_size = datas.shape[0]\n",
    "    print(datas_size)\n",
    "    #for l in range(datas_size):\n",
    "    #    if type(datas['description'][l]) == str:\n",
    "    #        continue\n",
    "    #    elif math.isnan(datas['description'][l]):\n",
    "    #        datas.drop([l],axis=0,inplace=True)  #删除description为空的行\n",
    "    #datas = datas.reset_index(drop=True)  #从新将从0开始顺序标注序号\n",
    "    #datas_size = datas.shape[0] #删除后的数据量\n",
    "    \n",
    "    if mode=='train':\n",
    "        labels = [] #保存atis train.csv中一共有多少类\n",
    "        for line in range(datas_size):\n",
    "            if datas['target'][line] not in labels:          #注意这里的csv文件的标签栏默认为 'label',如有不同需更改\n",
    "                labels.append(datas['target'][line])\n",
    "    \n",
    "    '''将description外的列为nan值的替换为[UNK]或0,然后拼接'''\n",
    "    h2 = []\n",
    "    for l in range(datas_size):\n",
    "        if type(datas['description'][l]) == str:\n",
    "            a=1\n",
    "        elif math.isnan(datas['description'][l]):\n",
    "            datas['description'][l] = ''\n",
    "            \n",
    "        if type(datas['neighbourhood'][l]) == str:\n",
    "            a=1\n",
    "        elif math.isnan(datas['neighbourhood'][l]):\n",
    "            datas['neighbourhood'][l] = '[UNK]'\n",
    "    \n",
    "        if type(datas['latitude'][l]) == str:\n",
    "            a=1\n",
    "        elif math.isnan(datas['latitude'][l]):\n",
    "            datas['latitude'][l] = 0\n",
    "        \n",
    "        if type(datas['longitude'][l]) == str:\n",
    "            a=1\n",
    "        elif math.isnan(datas['longitude'][l]):\n",
    "            datas['longitude'][l] = 0\n",
    "        \n",
    "        if type(datas['type'][l]) == str:\n",
    "            a=1\n",
    "        elif math.isnan(datas['type'][l]):\n",
    "            datas['type'][l] = '[UNK]'\n",
    "    \n",
    "        if type(datas['accommodates'][l]) == str:\n",
    "            a=1\n",
    "        elif math.isnan(datas['accommodates'][l]):\n",
    "            datas['accommodates'][l] = 0\n",
    "        \n",
    "        if type(datas['bathrooms'][l]) == str:\n",
    "            a=1\n",
    "        elif math.isnan(datas['bathrooms'][l]):\n",
    "            datas['bathrooms'][l] = '[UNK]'\n",
    "        \n",
    "        if type(datas['bedrooms'][l]) == str:\n",
    "            a=1\n",
    "        elif math.isnan(datas['bedrooms'][l]):\n",
    "            datas['bedrooms'][l] = 0\n",
    "    \n",
    "        if type(datas['reviews'][l]) == str:\n",
    "            a=1\n",
    "        elif math.isnan(datas['reviews'][l]):\n",
    "            datas['reviews'][l] = 0\n",
    "    \n",
    "        if type(datas['review_rating'][l]) == str:\n",
    "            a=1\n",
    "        elif math.isnan(datas['review_rating'][l]):\n",
    "            datas['review_rating'][l] = 0\n",
    "        \n",
    "        if type(datas['review_scores_A'][l]) == str:\n",
    "            a=1\n",
    "        elif math.isnan(datas['review_scores_A'][l]):\n",
    "            datas['review_scores_A'][l] = 0\n",
    "    \n",
    "        if type(datas['review_scores_B'][l]) == str:\n",
    "            a=1\n",
    "        elif math.isnan(datas['review_scores_B'][l]):\n",
    "            datas['review_scores_B'][l] = 0\n",
    "    \n",
    "        if type(datas['review_scores_C'][l]) == str:\n",
    "            a=1\n",
    "        elif math.isnan(datas['review_scores_C'][l]):\n",
    "            datas['review_scores_C'][l] = 0\n",
    "        \n",
    "        if type(datas['review_scores_D'][l]) == str:\n",
    "            a=1\n",
    "        elif math.isnan(datas['review_scores_D'][l]):\n",
    "            datas['review_scores_D'][l] = 0\n",
    "    \n",
    "        if type(datas['instant_bookable'][l]) == str:\n",
    "            a=1\n",
    "        elif math.isnan(datas['instant_bookable'][l]):\n",
    "            datas['instant_bookable'][l] = '[UNK]'\n",
    "    \n",
    "        s = str((datas['neighbourhood'][l])) +' '+ str(int(datas['review_rating'][l])) +' ' + str(int((datas['latitude'][l]+33)*(-100))) +' '+ str(int((datas['longitude'][l]-151)*(100))) +' '+ str(datas['type'][l]) +' '+ str(datas['accommodates'][l]) +' '+ str(datas['bathrooms'][l]) +' '+ str(int(datas['bedrooms'][l])) +' '+ str(int(datas['reviews'][l])) +' '+ str(int(datas['review_rating'][l])) +' '+ str(int(datas['review_scores_A'][l])) +' '+ str(int(datas['review_scores_B'][l]))+' '+ str(int(datas['review_scores_C'][l])) +' '+ str(int(datas['review_scores_D'][l])) +' '+ str(datas['instant_bookable'][l])\n",
    "        h2.append(s)\n",
    "    \n",
    "    train_text = []#存放训练数据中未进行tokenize的text\n",
    "    labels_idx = []\n",
    "    for l in range(datas_size):\n",
    "        h = datas['amenities'][l]  \n",
    "        h = h.strip('[]')  \n",
    "        h = re.sub('[\",]', '', h)  \n",
    "    \n",
    "        train_text.append('[CLS] '+ h+ ' '+ h2[l] + ' ' + datas['description'][l]+ h +' [SEP]')\n",
    "        if mode=='train':\n",
    "            labels_idx.append(datas['target'][l]) \n",
    "        \n",
    "    tokenizer = BertTokenizer.from_pretrained('./bert-pretrained') \n",
    "    train_data_tokens = [] \n",
    "    for l in range(datas_size):\n",
    "        tokens = tokenizer.tokenize(train_text[l])\n",
    "        train_data_tokens.append(tokens)\n",
    "    \n",
    "    max_len=0   \n",
    "    for i in range(datas_size):\n",
    "        max_len = max(max_len, len(train_data_tokens[i]))\n",
    "    if max_len>300:\n",
    "        max_len = 300  \n",
    "\n",
    "    train_tokens_idx=[]\n",
    "    for i in range(datas_size):\n",
    "        token_idx = tokenizer.convert_tokens_to_ids(train_data_tokens[i])\n",
    "        if len(token_idx) > max_len:\n",
    "            token_idx = token_idx[0:max_len] \n",
    "        while len(token_idx) < max_len:\n",
    "            token_idx.append(0)               \n",
    "        train_tokens_idx.append(token_idx)\n",
    "    \n",
    "    if mode=='train':\n",
    "        randnum = random.randint(0,100)\n",
    "        random.seed(randnum)\n",
    "        random.shuffle(train_tokens_idx)\n",
    "        random.seed(randnum)\n",
    "        random.shuffle(labels_idx)  #打乱训练集数据,这里注意必须打乱list类型的数据集，torch类型会导致重复\n",
    "        print(\"打乱数据集完成\")\n",
    "    \n",
    "    if mode=='train':\n",
    "        tensor_datasets = TensorDataset(torch.tensor(train_tokens_idx), torch.tensor(labels_idx))\n",
    "        train_tensor_datas = DataLoader(tensor_datasets, batch_size=256, shuffle=True, drop_last=True, num_workers=2)\n",
    "    elif mode=='eval':\n",
    "        tensor_datasets = TensorDataset(torch.tensor(train_tokens_idx))\n",
    "        train_tensor_datas = DataLoader(tensor_datasets, batch_size=256, shuffle=False, drop_last=False, num_workers=2)\n",
    "    \n",
    "    return train_tensor_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据量：12724\n"
     ]
    }
   ],
   "source": [
    "'''从csv文件读取数据'''\n",
    "datas = pd.read_csv('./data/aml/train.csv')\n",
    "datas_size = datas.shape[0]\n",
    "print(\"数据量：\" + str(datas_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "删除空评论行后的数据量：12314\n"
     ]
    }
   ],
   "source": [
    "'''删除掉description为空的行,如果不删除就跳过这行，如果要删除description空行就把之前data_init中前面注释掉的代码取消注释'''\n",
    "for l in range(datas_size):\n",
    "    if type(datas['description'][l]) == str:\n",
    "        continue\n",
    "    elif math.isnan(datas['description'][l]):\n",
    "        datas.drop([l],axis=0,inplace=True)  #删除description为空的行\n",
    "datas = datas.reset_index(drop=True)  #从新将从0开始顺序标注序号\n",
    "datas_size = datas.shape[0] #删除后的数据量\n",
    "print(\"删除空评论行后的数据量：\" + str(datas_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''将description外的列为nan值的替换为[UNK]或0,然后拼接'''\n",
    "h2 = []\n",
    "for l in range(datas_size):\n",
    "    if type(datas['description'][l]) == str:\n",
    "            a=1\n",
    "    elif math.isnan(datas['description'][l]):\n",
    "            datas['description'][l] = ''\n",
    "    if type(datas['neighbourhood'][l]) == str:\n",
    "        a=1\n",
    "    elif math.isnan(datas['neighbourhood'][l]):\n",
    "        datas['neighbourhood'][l] = '[UNK]'\n",
    "    \n",
    "    if type(datas['latitude'][l]) == str:\n",
    "        a=1\n",
    "    elif math.isnan(datas['latitude'][l]):\n",
    "        datas['latitude'][l] = 0\n",
    "        \n",
    "    if type(datas['longitude'][l]) == str:\n",
    "        a=1\n",
    "    elif math.isnan(datas['longitude'][l]):\n",
    "        datas['longitude'][l] = 0\n",
    "        \n",
    "    if type(datas['type'][l]) == str:\n",
    "        a=1\n",
    "    elif math.isnan(datas['type'][l]):\n",
    "        datas['type'][l] = '[UNK]'\n",
    "    \n",
    "    if type(datas['accommodates'][l]) == str:\n",
    "        a=1\n",
    "    elif math.isnan(datas['accommodates'][l]):\n",
    "        datas['accommodates'][l] = 0\n",
    "        \n",
    "    if type(datas['bathrooms'][l]) == str:\n",
    "        a=1\n",
    "    elif math.isnan(datas['bathrooms'][l]):\n",
    "        datas['bathrooms'][l] = '[UNK]'\n",
    "        \n",
    "    if type(datas['bedrooms'][l]) == str:\n",
    "        a=1\n",
    "    elif math.isnan(datas['bedrooms'][l]):\n",
    "        datas['bedrooms'][l] = 0\n",
    "    \n",
    "    if type(datas['reviews'][l]) == str:\n",
    "        a=1\n",
    "    elif math.isnan(datas['reviews'][l]):\n",
    "        datas['reviews'][l] = 0\n",
    "    \n",
    "    if type(datas['review_rating'][l]) == str:\n",
    "        a=1\n",
    "    elif math.isnan(datas['review_rating'][l]):\n",
    "        datas['review_rating'][l] = 0\n",
    "        \n",
    "    if type(datas['review_scores_A'][l]) == str:\n",
    "        a=1\n",
    "    elif math.isnan(datas['review_scores_A'][l]):\n",
    "        datas['review_scores_A'][l] = 0\n",
    "    \n",
    "    if type(datas['review_scores_B'][l]) == str:\n",
    "        a=1\n",
    "    elif math.isnan(datas['review_scores_B'][l]):\n",
    "        datas['review_scores_B'][l] = 0\n",
    "    \n",
    "    if type(datas['review_scores_C'][l]) == str:\n",
    "        a=1\n",
    "    elif math.isnan(datas['review_scores_C'][l]):\n",
    "        datas['review_scores_C'][l] = 0\n",
    "        \n",
    "    if type(datas['review_scores_D'][l]) == str:\n",
    "        a=1\n",
    "    elif math.isnan(datas['review_scores_D'][l]):\n",
    "        datas['review_scores_D'][l] = 0\n",
    "    \n",
    "    if type(datas['instant_bookable'][l]) == str:\n",
    "        a=1\n",
    "    elif math.isnan(datas['instant_bookable'][l]):\n",
    "        datas['instant_bookable'][l] = '[UNK]'\n",
    "    \n",
    "    s = str((datas['neighbourhood'][l])) +' '+ str(int(datas['review_rating'][l])) +' ' + str(int((datas['latitude'][l]+33)*(-100))) +' '+ str(int((datas['longitude'][l]-151)*(100))) +' '+ str(datas['type'][l]) +' '+ str(datas['accommodates'][l]) +' '+ str(datas['bathrooms'][l]) +' '+ str(int(datas['bedrooms'][l])) +' '+ str(int(datas['reviews'][l])) +' '+ str(int(datas['review_rating'][l])) +' '+ str(int(datas['review_scores_A'][l])) +' '+ str(int(datas['review_scores_B'][l]))+' '+ str(int(datas['review_scores_C'][l])) +' '+ str(int(datas['review_scores_D'][l])) +' '+ str(datas['instant_bookable'][l])\n",
    "    h2.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集的标签集合：[1, 0, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "'''统计一共有多少种标签'''\n",
    "labels = [] #保存atis train.csv中一共有多少类\n",
    "for line in range(datas_size):\n",
    "    if datas['target'][line] not in labels:          #注意这里的csv文件的标签栏默认为 'label',如有不同需更改\n",
    "            labels.append(datas['target'][line])\n",
    "print(\"数据集的标签集合：\"+ str(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''存放评论以及对应的标签'''\n",
    "train_text = []#存放训练数据中未进行tokenize的text\n",
    "labels_idx = []\n",
    "for l in range(datas_size):\n",
    "    h = datas['amenities'][l]  #将amenities的字符拼接到description\n",
    "    h = h.strip('[]')  #删除括号\n",
    "    h = re.sub('[\",]', '', h)  #删除引号和逗号\n",
    "    \n",
    "    train_text.append('[CLS] '+ h + ' '+ h2[l] +' '+ datas['description'][l] +' [SEP]')\n",
    "    labels_idx.append(datas['target'][l]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据中最长的token长度为：300\n",
      "分词效果示例：['[CLS]', 'wash', '##er', 'free', 'parking', 'on', 'premises', 'long', 'term', 'stays', 'allowed', 'hot', 'water', 'air', 'conditioning', 'dry', '##er', 'essential', '##s', 'kitchen', 'hang', '##ers', 'sham', '##poo', 'hair', 'dry', '##er', 'wi', '##fi', 'heating', 'rock', '##dale', '80', '95', '14', 'entire', 'home', '/', 'apt', '4', '1', 'bath', '2', '1', '80', '6', '10', '8', '10', 't', 'really', 'quite', 'area', 'and', 'very', 'clean', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "'''得到每个token的index值，并做padding,也可以用一个mask的list，如果是[pad]符号则为0，其他token为1，与原始的token序列连接'''\n",
    "tokenizer = BertTokenizer.from_pretrained('./bert-pretrained') #必须要./表示当前文件夹的某个文件\n",
    "train_data_tokens = [] #用于存放tokenize后的训练数据tokens\n",
    "for l in range(datas_size):\n",
    "    tokens = tokenizer.tokenize(train_text[l])\n",
    "    train_data_tokens.append(tokens)\n",
    "    \n",
    "max_len=0   #训练数据中最大的token长度进行padding时候用\n",
    "for i in range(datas_size):\n",
    "    max_len = max(max_len, len(train_data_tokens[i]))\n",
    "if max_len>300:\n",
    "    max_len = 300  #bert最大编码长度为512\n",
    "print('训练数据中最长的token长度为：'+str(max_len))\n",
    "\n",
    "train_tokens_idx=[]\n",
    "for i in range(datas_size):\n",
    "    token_idx = tokenizer.convert_tokens_to_ids(train_data_tokens[i])\n",
    "    if len(token_idx) > max_len:\n",
    "        token_idx = token_idx[0:max_len] \n",
    "    while len(token_idx) < max_len:\n",
    "        token_idx.append(0)                  #bert的[pad]对应的index为0，所以添加0做padding\n",
    "    train_tokens_idx.append(token_idx)\n",
    "    \n",
    "print('分词效果示例：'+ str(train_data_tokens[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''随机打乱训练集数据，防止学习到数据本身的顺序特征, 并将数据装入Dataloader中'''\n",
    "randnum = random.randint(0,100)\n",
    "random.seed(randnum)\n",
    "random.shuffle(train_tokens_idx)\n",
    "random.seed(randnum)\n",
    "random.shuffle(labels_idx)  #打乱训练集数据,这里注意必须打乱list类型的数据集，torch类型会导致重复\n",
    "\n",
    "tensor_datasets = TensorDataset(torch.tensor(train_tokens_idx), torch.tensor(labels_idx))\n",
    "train_tensor_datas = DataLoader(tensor_datasets, batch_size=256, shuffle=True, drop_last=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''定义模型类\n",
    "参数\n",
    "hidden_size: bert的embedding size\n",
    "xlnet_hidden_dim:gru 隐藏层维度\n",
    "xlnet_n_layers: gru层数\n",
    "xlnet_bidirectional :gru是否双向\n",
    "xlnet_dropout :gru dropout大小\n",
    "num_classes:类数目\n",
    "'''\n",
    "class Bert_GRU(nn.Module):\n",
    "    def __init__(self, hidden_size, xlnet_hidden_dim, xlnet_n_layers, xlnet_bidirectional, xlnet_dropout, num_classes):\n",
    "        \n",
    "        super(Bert_GRU,self).__init__()\n",
    "        \n",
    "        self.bert=BertModel.from_pretrained('./bert-pretrained')  \n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False  \n",
    "        \n",
    "        self.rnn = nn.GRU(hidden_size,\n",
    "                          xlnet_hidden_dim,\n",
    "                          num_layers = xlnet_n_layers,\n",
    "                          bidirectional = xlnet_bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if xlnet_n_layers < 2 else xlnet_dropout)\n",
    "        \n",
    "        self.fc1 = nn.Linear(xlnet_hidden_dim * 2 if xlnet_bidirectional else xlnet_hidden_dim, 512)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(xlnet_dropout)\n",
    "        \n",
    "    def forward(self, tokens):\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            encoder_out,pooled = self.bert(tokens,output_all_encoded_layers=False) \n",
    "        _, hidden = self.rnn(encoder_out)\n",
    "        \n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "        hidden = self.fc1(hidden)\n",
    "        hidden = F.relu(hidden)\n",
    "        output = self.fc(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''BERT+CNN'''\n",
    "class Bert_CNN(nn.Module):\n",
    "    def __init__(self,num_filters, hidden_size, filter_size, dropout, num_classes):\n",
    "        super(Bert_CNN,self).__init__()\n",
    "        self.bert=BertModel.from_pretrained('./bert-pretrained')  \n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False \n",
    "            \n",
    "        self.convs=nn.ModuleList(\n",
    "\n",
    "            [nn.Conv2d(1,num_filters,(k,hidden_size)) for k in filter_size]   \n",
    "        )\n",
    "\n",
    "        \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_filters*len(filter_size), 512)\n",
    "        self.fc = nn.Linear(512, num_classes ) \n",
    "\n",
    "    def conv_and_pool(self, cnn_in, conv2d):\n",
    "        cnn_in=conv2d(cnn_in)   \n",
    "        cnn_in=F.relu(cnn_in)     \n",
    "        cnn_in=cnn_in.squeeze(3)            \n",
    "        cnn_in=F.max_pool1d(cnn_in, cnn_in.size(2))\n",
    "      \n",
    "        cnn_in = cnn_in.squeeze(2)  \n",
    "  \n",
    "        return cnn_in\n",
    "\n",
    "    def forward(self, tokens):\n",
    "  \n",
    "        encoder_out,pooled = self.bert(tokens,output_all_encoded_layers=False) \n",
    "        cnn_in = encoder_out.unsqueeze(1)  \n",
    "        cnn_out = torch.cat([self.conv_and_pool(cnn_in, conv2d) for conv2d in self.convs],1) \n",
    "        cnn_out = self.fc1(cnn_out)\n",
    "        cnn_out = F.relu(cnn_out)\n",
    "        cnn_out = self.dropout(cnn_out)\n",
    "        out=self.fc(cnn_out) \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''定义训练函数'''\n",
    "def train(model, lossfunc, optimizer, epochs, tensor_datas): #增加了需要自己输入的epochs\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.train()\n",
    "\n",
    "    model = model.to(device)\n",
    "    losses = [] \n",
    "    accuracies = []\n",
    "    iter = [] \n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    for epoch in range(epochs):\n",
    "        loss_sum = 0\n",
    "        accuracy=0\n",
    "        for idx, datas in enumerate(tqdm(tensor_datas)):  \n",
    "            tokens = datas[0].to(device)\n",
    "            labels = datas[1].to(device)\n",
    "        \n",
    "            optimizer.zero_grad() \n",
    "            probs = model(tokens).squeeze()  \n",
    "            probs.squeeze()\n",
    "            \n",
    "            loss = lossfunc(probs, labels) \n",
    "            loss_sum += loss.item()\n",
    "            loss.backward()\n",
    "            \n",
    "            accuracy += (labels == torch.argmax(probs, dim=1)).sum()  #计算预测标签和真实标签相等的数量\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()#学习率递减\n",
    "        accuracy = accuracy / ((idx+1)*tensor_datas.batch_size)\n",
    "        \n",
    "        accuracies.append(accuracy.item()) \n",
    "        losses.append(loss_sum)\n",
    "        iter.append(epoch)\n",
    "        print(\"the loss of  training data \"+ str(epoch) + \"  \" + str(loss_sum))\n",
    "        print(\"the accuracy of training data   \"+ str(epoch) + \"  \" + str(accuracy))\n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.title(\"Losses\")\n",
    "    plt.xlabel(\"loss per epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(iter, losses)\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.title(\"accuracies\")\n",
    "    plt.xlabel(\"ccuracy per epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.plot(iter, accuracies)\n",
    "\n",
    "    plt.show()\n",
    "    return accuracies, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''定义模型，损失函数，优化器'''\n",
    "model = Bert_GRU(768, 256, 2, True, 0.5, 6)\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "lossfuc = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Bert_CNN(256, 768, (2,3,4), 0.5,6)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "lossfuc = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies, losses = train(model, lossfuc, optimizer, 7, train_tensor_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_datas = pd.read_csv('./data/aml/valid.csv')\n",
    "valid_datas = data_init(valid_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(tensor_datas, model):\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    accuracy=0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, datas in enumerate(tqdm(tensor_datas)):  \n",
    "            tokens = datas[0].to(device)\n",
    "            labels = datas[1].to(device)\n",
    "            probs = model(tokens).squeeze()\n",
    "            probs = F.softmax(probs, dim=1)\n",
    "            accuracy += (labels == torch.argmax(probs, dim=1)).sum()  #计算预测标签和真实标签相等的数量\n",
    "    accuracy = accuracy / ((idx+1)*tensor_datas.batch_size)\n",
    "    print(accuracy)\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:27<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5430, device='cuda:1')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.54296875"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(valid_datas, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''将全部训练数据（没划分验证集的）训练选择的模型BERtCNN'''\n",
    "train_datas = pd.read_csv('./data/aml/train_origin.csv')\n",
    "train_datas = data_init(train_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies, losses = train(model, lossfuc, optimizer, 5, train_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [02:43<00:00,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5898, device='cuda:1')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5897763967514038"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(train_datas, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''放入测试举数据到Dataloader中'''\n",
    "test_datas = pd.read_csv('./data/aml/test.csv')\n",
    "test_datas = data_init(test_datas, mode='eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(tensor_datas, model):\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    accuracy=0\n",
    "    model.eval()\n",
    "    labels_pred = torch.tensor([])\n",
    "    with torch.no_grad():\n",
    "        for idx, datas in enumerate(tqdm(tensor_datas)):  \n",
    "            tokens = datas[0].to(device)\n",
    "            \n",
    "            probs = model(tokens).squeeze()\n",
    "            probs = F.softmax(probs, dim=1)\n",
    "            labels_pred = torch.cat([labels_pred, torch.argmax(probs, dim=1).to('cpu')])  \n",
    "    \n",
    "    return labels_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:55<00:00,  2.79s/it]\n"
     ]
    }
   ],
   "source": [
    "'''得到测试集的预测标签'''\n",
    "labels_pred = test(test_datas, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.,  ..., 0., 1., 0.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pred_np = labels_pred.numpy()\n",
    "labels_pred_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''预测结果放入txt文件'''\n",
    "import numpy as np\n",
    "np.savetxt('MG21330006.txt',labels_pred_np,fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里如果不知道为啥将生成的txt文件下载到本地会乱码，所以直接复制内容新建txt文件黏贴进去就行了"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a640cb57adece9e07f6e8781c1aa5f9f518b98ca1c6f12075c83549c4423c7de"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('torch1.4py3.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
